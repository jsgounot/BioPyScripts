import glob, os

# Read this
# https://github.com/jsgounot/metagenomic-pipelines/wiki/Reads-compression
# NOTE : This script might use a lot of disk space during runtime (multiple file copies for check purpose)
# This could be improved by adding a conditionnal rule during the checking analysis
# but this is not something easily done with snakemake
# Look Spring option for long reads !

configfile: "config.json"

internal = {}
for sample, sreads in config.items():
    internal[sample] = {}
    for rid, rdata in sreads.items():
        if isinstance(rdata, list):
            if any(read.startswith('ftp') or read.startswith('http') for read in rdata):
                raise Exception('List with url is not (yet) supported')
            internal[sample][rid] = f'concat/{sample}.{rid}.fastq.gz'
        elif isinstance(rdata, str):
            if rdata.startswith('ftp') or rdata.startswith('http'):
                internal[sample][rid] = f'download/{sample}.{rid}.fastq.gz'
            else:
                internal[sample][rid] = rdata
        else:
            raise Exception('Read data can be either a string (path or url) or a list of strings')

rule all:
    input:
        expand("spring_files/{name}.spring", name=list(config)),
        "output/compression.check.txt",
        "output/compression.stat.tsv"

rule wget_read:
    params:
        url = lambda wc : config[wc.name][wc.rid]
    output:
        temp('download/{name}.{rid}.fastq.gz')
    shell:
        'wget {params.url} --quiet -O {output}'

rule concat:
    input:
        lambda wc : config[wc.name][wc.rid]
    output:
        temp('concat/{name}.{rid}.fastq.gz')
    shell:
        'cat {input} > {output}'

rule compress:
    input:
        r1 = lambda wc: internal[wc.name]['r1'],
        r2 = lambda wc: internal[wc.name]['r2']
    output:
        "spring_files/{name}.spring"
    threads:
        8
    group:
        "main"
    shell:
        "spring -c -g -t {threads} -i {input.r1} {input.r2} -o {output}"

rule sample_size:
    input:
        r1 = lambda wc: internal[wc.name]['r1'],
        r2 = lambda wc: internal[wc.name]['r2'],
        co = rules.compress.output
    output:
        'output/{name}.dustat.txt'
    shell:
        'du {input.r1} {input.r2} {input.co} > {output}'

rule uncompress:
    input:
        rules.compress.output
    output:
        r1 = temp("temporary/{name}.r1.fastq.gz"),
        r2 = temp("temporary/{name}.r2.fastq.gz")
    threads:
        8
    group:
        "main"
    shell:
        "spring -d -g -t {threads} -i {input} -o {output.r1} {output.r2}"

checkpoint raw_cmp:
    input:
        f1 = "temporary/{name}.{rid}.fastq.gz",
        f2 = lambda wc : internal[wc.name][wc.rid]
    output:
        "check/{name}.{rid}.direct.zcmp"
    shell:
        # We need to add `|| true` at the end since zcmp return 
        # exit code 1 if diff is found which make snakemake stop
        "zcmp {input.f1} {input.f2} > {output} || true"

# ------------------------------------------------------------------------
# Case 2: Fastq gz files are NOT directly the same
# Might be due to optional line 3 differences
# Need to awk both fastq.gz and zcmp again

rule awk_spring:
    input:
        "temporary/{name}.{rid}.fastq.gz"
    output:
        temp("temporary/{name}.{rid}.spring.awked.gz")
    shell:
        "awk '(NR%4!=3)' <(gzip -dc {input}) | gzip > {output}"

rule awk_original:
    input:
        lambda wc: internal[wc.name][wc.rid]
    output:
        temp("temporary/{name}.{rid}.original.awked.gz")
    shell:
        "awk '(NR%4!=3)' <(gzip -dc {input}) | gzip > {output}"

rule check_cmp:
    input:
        f1 = rules.awk_spring.output,
        f2 = rules.awk_original.output
    output:
        "check/{name}.{rid}.awk.zcmp"
    shell:
        "zcmp {input.f1} {input.f2} > {output}"

# ------------------------------------------------------------------------
# Aggregation to check if we need to go through case #2 

def aggregate_cmp(wc):
    # input function for the rule aggregate
    # https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#data-dependent-conditional-execution
    # in this context we want to run the awk cmp only if gcmp is not good
    # which can happen if the optional identifier vary between the uncompressed and default read files

    with checkpoints.raw_cmp.get(name=wc.name, rid=wc.rid).output[0].open() as f:
        content = f.read().strip()
        if not content:
            print ('empty', wc.name, wc.rid, content)
            return "check/{name}.{rid}.direct.zcmp"
        else:
            print ('not empty, run awk:', wc.name, wc.rid, content)
            return "check/{name}.{rid}.awk.zcmp"

rule mv_cmp:
    input:
        aggregate_cmp
    output:
        "check/{name}.{rid}.cmp"
    priority:
        1
    shell:
        "cp {input} {output}"

rule compression_check:
    input:
        expand("check/{name}.{rid}.cmp", name=list(config), rid=('r1', 'r2'))
    output:
        "output/compression.check.txt"
    script:
        "compression_check.py"

rule make_table:
    input:
        expand("output/{name}.dustat.txt", name=config)
    output:
        "output/compression.stat.tsv"
    params:
        config = config
    script:
        "spring_compression_stat.py"
